{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this notebook is to demonstrate how to use the cross validation library on datasets in order to  achieve accurte results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~\n"
     ]
    }
   ],
   "source": [
    "\\l ../../ml.q\n",
    ".ml.loadfile`:init.q\n",
    "\\l graphics.q\n",
    "\\c 15 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading library scripts and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell the functions related to the FRESH library are loaded in the 1st line while preprocessing functions used within the notebook are loaded from the folder mlutils. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is taken from the \"Breast Cancer Wisconsin (Diagnostic) Data Set\" to predict whether a mass of cells is malignant or benign.\n",
    "\n",
    "The diagnosis column is removed from the dataset and used as the target as this is what we will be predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean co..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "842302      17.99       10.38        122.8          1001      0.1184          0.2776           0...\n",
       "842517      20.57       17.77        132.9          1326      0.08474         0.07864          0...\n",
       "8.43009e+07 19.69       21.25        130            1203      0.1096          0.1599           0...\n",
       "8.43483e+07 11.42       20.38        77.58          386.1     0.1425          0.2839           0...\n",
       "8.43584e+07 20.29       14.34        135.1          1297      0.1003          0.1328           0...\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"The dataset contains 569 patient files\"\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data:(\"FSFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\"; enlist \",\") 0:`:datasets/data.csv\n",
    "targets:select diagnosis from data\n",
    "5#data:delete diagnosis from data\n",
    "\n",
    "\"The dataset contains \",(string count data),\" patient files\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is used on the target data to convert symbols into a numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets:exec diagnosis_M from .ml.util.onehot[targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we produce polynomial features from our data in order to allow for interactions between terms in the system to also be studied not just the individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean co..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "842302      17.99       10.38        122.8          1001      0.1184          0.2776           0...\n",
       "842517      20.57       17.77        132.9          1326      0.08474         0.07864          0...\n",
       "8.43009e+07 19.69       21.25        130            1203      0.1096          0.1599           0...\n",
       "8.43483e+07 11.42       20.38        77.58          386.1     0.1425          0.2839           0...\n",
       "8.43584e+07 20.29       14.34        135.1          1297      0.1003          0.1328           0...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/ Add 2nd order polynomial features to the table \n",
    "5#table:data^.ml.util.polytab[flip 1_flip data;2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id   | absenergy_radius_mean absenergy_texture_mean absenergy_perimeter_mean absenergy_area_mean ..\n",
      "-----| ------------------------------------------------------------------------------------------..\n",
      "8670 | 239.0116              379.4704               10342.89                 560851.2            ..\n",
      "8913 | 166.1521              172.1344               6705.972                 266152.8            ..\n",
      "8915 | 223.8016              364.81                 9414.821                 472381.3            ..\n",
      "9047 | 167.4436              261.4689               6918.912                 257657.8            ..\n",
      "85715| 173.4489              348.1956               7392.56                  285797.2            ..\n",
      "86208| 410.4676              530.3809               17529.76                 1597696             ..\n",
      "86211| 148.3524              318.2656               6051.284                 203491.2            ..\n",
      "86355| 495.9529              386.9089               23347.84                 2277081             ..\n",
      "86408| 159.5169              430.9776               6748.623                 230784.2            ..\n",
      "86409| 203.3476              386.1225               9570.709                 396774              ..\n",
      "..\n",
      "The forecasting frame contains 569 datapoints.\n"
     ]
    }
   ],
   "source": [
    "dict:.ml.i.dict\n",
    "/ in this example we look only at features of the data alone with no parameters\n",
    "show tabraw:.ml.fresh.createfeatures[data;`id;1_cols data;dict]\n",
    "-1\"The forecasting frame contains \",(string count tabraw),\" datapoints.\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in any cells in the data set that might have nulls in it by using the average of that column. An extra column is also added noting if there previously was a null in the original position. Any constant columns are also dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "absenergy_radius_mean absenergy_texture_mean absenergy_perimeter_mean absenergy_area_mean absener..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "239.0116              379.4704               10342.89                 560851.2            0.01192..\n",
       "166.1521              172.1344               6705.972                 266152.8            0.00483..\n",
       "223.8016              364.81                 9414.821                 472381.3            0.00808..\n",
       "167.4436              261.4689               6918.912                 257657.8            0.00975..\n",
       "173.4489              348.1956               7392.56                  285797.2            0.01340..\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabraw:.ml.util.nullencode[value tabraw;avg]\n",
    "5#tabraw:.ml.util.dropconstant[tabraw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data must now be converted to a matrix from a table in order to allow it to be passed to a machine learning algorithm for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mattab:{flip value flip x}\n",
    "/ Convert the table containing significant features to a matrix in order to allow it to be passed to a machine learning algorithm\n",
    "featmat:mattab[tabraw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is the process of finding optimal parameters for a given model. This works by applying a model to a dataset while also giving a dictionary with various parameters for the model. The gridserch applies various combinations of these parameters, as a results it returns the optimal parameters for the model along with the accuracy result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr:.p.import[`sklearn.tree][`:DecisionTreeClassifier]\n",
    "dict:`max_depth`min_samples_split`max_features!(1_til 5;2_til 6;`auto`log2`sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the indices in ascending order of the dataset partitioned into k subsections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "i:.ml.xval.kfsplit[targets;3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6324608\n",
       "`max_depth`min_samples_split`max_features!(,1;,4;,`log2)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".ml.xval.gridsearch[featmat;targets;i;regr;dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the results from the grid search we can apply this to the Random Forest Classifier model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf:.p.import[`sklearn.tree][`:DecisionTreeClassifier\n",
    "    ][`max_depth pykw 1;`min_samples_split pykw 4;`max_features pykw `log2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-forward, Monte Carlo and Repeated Stratified Randomised k-fold cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain forward is a useful cross validation technique where the data is split into equi-sized bins with increasing amounts of the data incorporated in the testing set at each step. \n",
    "\n",
    "The Monte Carlo method involves randomly selecting a certain amount of the data, using the rest as the test set.  This process is then repeated a number of times, taking different random data as the training and testing set each time. \n",
    "\n",
    "In k-fold cross validation, the data is split up into k equal size sections. One fold is kept for validation while the other k-1 folds are used for training. The process is repeated k times so that each iteration a different fold is used as validation. Stratification is the process in which the dataset is arranged so that each fold has a good representation of the dataset. This is used xtensively in cases where the distribution of classes in the data is unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation score using chain-forward is 0.504386\n",
      "The cross validation score using monte-carlo is 0.6345029\n",
      "The cross validation score using Repeated Stratified Randomized K-fold is 0.6212534\n"
     ]
    }
   ],
   "source": [
    "chainfor:.ml.xval.chainxval[featmat;targets;5;clf]\n",
    "montecarlo:.ml.xval.mcxval[featmat;targets;.2;clf;3]\n",
    "repkf:.ml.xval.repkfstrat[featmat;targets;2;2;clf]\n",
    "\n",
    "-1\"The cross validation score using chain-forward is \",string chainfor;\n",
    "-1\"The cross validation score using monte-carlo is \",string montecarlo;\n",
    "-1\"The cross validation score using Repeated Stratified Randomized K-fold is \",string repkf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example it is evident that the Monte Carlo and Repeated Stratified Randomised k-fold method gives the most accurate results as chain-forward cross validation is most useful on timeseries data. This shows how in this case, repeatedly splitting the input and target data at random leads to high cross validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
